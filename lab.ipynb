{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a98806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3caca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 15:05:27 WARN Utils: Your hostname, guilhermerc-desktop resolves to a loopback address: 127.0.1.1; using 192.168.2.157 instead (on interface enp5s0)\n",
      "25/04/28 15:05:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/28 15:05:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do Spark: 3.5.5\n",
      "URL do Master: local[*]\n",
      "Aplicativo: DataFrame Example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|  valor|\n",
      "+---+-------+\n",
      "|  1|  teste|\n",
      "|  2|exemplo|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Configuração do SparkSession com mais opções\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Teste para verificar se a conexão está funcionando\n",
    "print(f\"Versão do Spark: {spark.version}\")\n",
    "print(f\"URL do Master: {spark.sparkContext.master}\")\n",
    "print(f\"Aplicativo: {spark.sparkContext.appName}\")\n",
    "\n",
    "# Crie um DataFrame simples para testar\n",
    "df = spark.createDataFrame([(1, \"teste\"), (2, \"exemplo\")], [\"id\", \"valor\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627dc0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando tabelas para o catálogo e contando registros...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 15:05:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/04/28 15:05:41 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela 'call_center' carregada com sucesso: 24 registros\n",
      "Tabela 'catalog_page' carregada com sucesso: 12000 registros\n",
      "Tabela 'catalog_returns' carregada com sucesso: 1439882 registros\n",
      "Tabela 'catalog_sales' carregada com sucesso: 14400425 registros\n",
      "Tabela 'customer' carregada com sucesso: 500000 registros\n",
      "Tabela 'customer_address' carregada com sucesso: 250000 registros\n",
      "Tabela 'customer_demographics' carregada com sucesso: 1920800 registros\n",
      "Tabela 'date_dim' carregada com sucesso: 73049 registros\n",
      "Tabela 'household_demographics' carregada com sucesso: 7200 registros\n",
      "Tabela 'income_band' carregada com sucesso: 20 registros\n",
      "Tabela 'inventory' carregada com sucesso: 133110000 registros\n",
      "Tabela 'item' carregada com sucesso: 102000 registros\n",
      "Tabela 'promotion' carregada com sucesso: 500 registros\n",
      "Tabela 'reason' carregada com sucesso: 45 registros\n",
      "Tabela 'ship_mode' carregada com sucesso: 20 registros\n",
      "Tabela 'store' carregada com sucesso: 102 registros\n",
      "Tabela 'store_returns' carregada com sucesso: 2877632 registros\n",
      "Tabela 'store_sales' carregada com sucesso: 28800501 registros\n",
      "Tabela 'time_dim' carregada com sucesso: 86400 registros\n",
      "Tabela 'warehouse' carregada com sucesso: 10 registros\n",
      "Tabela 'web_page' carregada com sucesso: 200 registros\n",
      "Tabela 'web_returns' carregada com sucesso: 720274 registros\n",
      "Tabela 'web_sales' carregada com sucesso: 7200088 registros\n",
      "Tabela 'web_site' carregada com sucesso: 42 registros\n",
      "\n",
      "Resumo das tabelas carregadas:\n",
      "------------------------------------------------------------\n",
      "Tabela                        | Registros\n",
      "------------------------------------------------------------\n",
      "call_center                   | 24\n",
      "catalog_page                  | 12000\n",
      "catalog_returns               | 1439882\n",
      "catalog_sales                 | 14400425\n",
      "customer                      | 500000\n",
      "customer_address              | 250000\n",
      "customer_demographics         | 1920800\n",
      "date_dim                      | 73049\n",
      "household_demographics        | 7200\n",
      "income_band                   | 20\n",
      "inventory                     | 133110000\n",
      "item                          | 102000\n",
      "promotion                     | 500\n",
      "reason                        | 45\n",
      "ship_mode                     | 20\n",
      "store                         | 102\n",
      "store_returns                 | 2877632\n",
      "store_sales                   | 28800501\n",
      "time_dim                      | 86400\n",
      "warehouse                     | 10\n",
      "web_page                      | 200\n",
      "web_returns                   | 720274\n",
      "web_sales                     | 7200088\n",
      "web_site                      | 42\n",
      "------------------------------------------------------------\n",
      "Total de tabelas carregadas: 24\n",
      "\n",
      "Tabelas disponíveis no catálogo:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Listar as tabelas disponíveis no catálogo\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTabelas disponíveis no catálogo:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistTables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "# Caminho para o dataset TPC-DS\n",
    "base_path = \"/home/guilhermerc/Documentos/workspace/pyspark-playground/book_data/tpcds_10\"\n",
    "\n",
    "# Listar todos os diretórios (tabelas) no caminho\n",
    "tables = []\n",
    "\n",
    "# Como estamos executando de fora do contêiner, precisamos listar as tabelas de outra maneira\n",
    "# Uma maneira é usando o código abaixo com o spark para listar as tabelas\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Lista de tabelas conhecidas do TPC-DS\n",
    "tpcds_tables = [\n",
    "    \"call_center\", \"catalog_page\", \"catalog_returns\", \"catalog_sales\", \n",
    "    \"customer\", \"customer_address\", \"customer_demographics\", \"date_dim\",\n",
    "    \"household_demographics\", \"income_band\", \"inventory\", \"item\",\n",
    "    \"promotion\", \"reason\", \"ship_mode\", \"store\", \"store_returns\",\n",
    "    \"store_sales\", \"time_dim\", \"warehouse\", \"web_page\", \"web_returns\",\n",
    "    \"web_sales\", \"web_site\"\n",
    "]\n",
    "\n",
    "# Dicionário para armazenar os resultados\n",
    "results = {}\n",
    "\n",
    "print(\"Carregando tabelas para o catálogo e contando registros...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Para cada tabela, carregue os dados e registre como tabela no catálogo\n",
    "for table in tpcds_tables:\n",
    "    table_path = f\"{base_path}/{table}\"\n",
    "    \n",
    "    # Tentativa de carregar o dataframe\n",
    "    try:\n",
    "        # Carregar a tabela como DataFrame\n",
    "        df = spark.read.parquet(table_path)\n",
    "        \n",
    "        # Registrar como tabela temporária\n",
    "        df.createOrReplaceTempView(table)\n",
    "        \n",
    "        # Contar os registros\n",
    "        count = df.count()\n",
    "        \n",
    "        # Armazenar o resultado\n",
    "        results[table] = count\n",
    "        \n",
    "        print(f\"Tabela '{table}' carregada com sucesso: {count} registros\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar tabela '{table}': {str(e)}\")\n",
    "\n",
    "# Exibir um resumo dos resultados\n",
    "print(\"\\nResumo das tabelas carregadas:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Tabela\".ljust(30) + \"| Registros\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for table, count in sorted(results.items()):\n",
    "    print(f\"{table.ljust(30)}| {count}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total de tabelas carregadas: {len(results)}\")\n",
    "\n",
    "# Listar as tabelas disponíveis no catálogo\n",
    "print(\"\\nTabelas disponíveis no catálogo:\")\n",
    "spark.catalog.listTables().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpcds_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
